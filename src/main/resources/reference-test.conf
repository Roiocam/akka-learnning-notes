akka {

  # 防止akka相关单元测试不能正常结束
  coordinated-shutdown.exit-jvm = off
  coordinated-shutdown.run-by-actor-system-terminate = off

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # stdout-loglevel = "OFF"
  stdout-loglevel = "ERROR"
  # loglevel = "OFF"
  # loglevel = "DEBUG"
  loglevel = "ERROR"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-dead-letters = on
  log-dead-letters-during-shutdown = off

  actor {
    allow-java-serialization = on

    serialization-bindings {
      "com.iquantex.phoenix.typedactor.guide.protocol.CborSerializable" = jackson-cbor
    }

    provider = "cluster"
    default-dispatcher {
      # Must be one of the following
      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
      # MessageDispatcherConfigurator with a public constructor with
      # both com.typesafe.config.Config parameter and
      # akka.dispatch.DispatcherPrerequisites parameters.
      # PinnedDispatcher must be used together with executor=thread-pool-executor.
      type = "Dispatcher"

      # Which kind of ExecutorService to use for this dispatcher
      # Valid options:
      #  - "default-executor" requires a "default-executor" section
      #  - "fork-join-executor" requires a "fork-join-executor" section
      #  - "thread-pool-executor" requires a "thread-pool-executor" section
      #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
      #  - A FQCN of a class extending ExecutorServiceConfigurator
      executor = "default-executor"

      # This will be used if you have set "executor = "default-executor"".
      # If an ActorSystem is created with a given ExecutionContext, this
      # ExecutionContext will be used as the default executor for all
      # dispatchers in the ActorSystem configured with
      # executor = "default-executor". Note that "default-executor"
      # is the default value for executor, and therefore used if not
      # specified otherwise. If no ExecutionContext is given,
      # the executor configured in "fallback" will be used.
      default-executor {
        fallback = "fork-join-executor"
      }

      # This will be used if you have set "executor = "fork-join-executor""
      # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 24

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 10.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 128

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
       # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }

      throughput = 32
    }
  }

  # For the sample, just bind to loopback and do not allow access from the network
  # the port is overridden by the logic in main class
  remote.artery {
    enabled = on
    transport = tcp
    canonical.port = 2551
    canonical.hostname = 127.0.0.1
  }

  cluster {
    seed-nodes = [
      "akka://test@127.0.0.1:2551",
    ]
    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # It will not add any extra safety for auto-down-unreachable-after, since that is not
    # handling network partitions.
    # Disable with "off" or specify a duration to enable.
    #
    # It is recommended to configure this to the same value as the stable-after property.
    down-removal-margin = 7s

    sharding {
      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        # The difference between number of shards in the region with most shards and
        # the region with least shards must be greater than (>) the `rebalanceThreshold`
        # for the rebalance to occur.
        # 1 gives the best distribution and therefore typically the best choice.
        # Increasing the threshold can result in quicker rebalance but has the
        # drawback of increased difference between number of shards (and therefore load)
        # on different nodes before rebalance will occur.
        # Default is 1 as of Akka 2.5.20
        rebalance-threshold = 1

        # The number of ongoing rebalancing processes is limited to this number.
        # Default is 3 as of Akka 2.5.20
        max-simultaneous-rebalance = 5
      }
    }
  }


}

akka.discovery {
  config.services = {
    local-cluster = {
      endpoints = [
        {
          host = "127.0.0.1"
          port = 8558
        }
      ]
    }
  }
}

akka.management {
  cluster.bootstrap {
    contact-point-discovery {
      discovery-method = config
    }
  }
}

